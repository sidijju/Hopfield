model:
  name: "linear_attention"         # Model type to use as backbone
  dim: 256            # Model embedding dimension
  num_layers: 2       # Number of layers in the model

training:
  batch_size: 4       # Batch size for training and evaluation
  epochs: 3           # Number of training epochs
  sequence_length: 8192 # Maximum sequence length 1024
  learning_rate: 5e-5 # Learning rate for optimizer

hardware:
  device: "cuda"      # Device to run on (cuda/cpu)

benchmark:
  name: "lra"     # Options: lambada, wikitext, memcopy, lra

logging:
  project_name: "language_model_benchmarks"
  